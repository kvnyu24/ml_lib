"""
Support Vector Machine Library
============================

A comprehensive library implementing advanced SVM functionality:

- Data generation, preprocessing and augmentation
- Multiple SVM variants and kernels
- Model evaluation, visualization and analysis
- Hyperparameter optimization
- Custom kernel development
- Online/incremental learning
- Multi-class classification strategies
- Probability calibration
- Feature selection and dimensionality reduction
- Advanced optimization algorithms
- Active learning mechanisms
- Ensemble methods
- Incremental/Online learning
- Sparse optimization

The implementation follows clean design principles with extensible interfaces.
"""

import numpy as np
from typing import Dict, List, Optional, Tuple, Union, Any, Callable
from dataclasses import dataclass
from abc import ABC, abstractmethod

from core import (
    Estimator, Optimizer, Loss,
    check_array, check_X_y, check_is_fitted,
    get_logger, TrainingLogger,
    Number, Array, Features, Target,
    EPSILON, DEFAULT_RANDOM_STATE
)

# Configure logging
logger = get_logger(__name__)

class BaseOptimizer(Optimizer):
    """Abstract base class for SVM optimizers."""
    
    @abstractmethod
    def optimize(self, X: Features, y: Target,
                kernel_fn: Callable, C: float) -> np.ndarray:
        """Optimize SVM parameters."""
        pass

class SMOOptimizer(BaseOptimizer):
    """Sequential Minimal Optimization."""
    
    def __init__(self, max_iter: int = 1000, tol: float = 1e-3):
        super().__init__(learning_rate=1.0)  # Learning rate not used in SMO
        self.max_iter = max_iter
        self.tol = tol
        
    def compute_update(self, params: np.ndarray, gradients: np.ndarray) -> np.ndarray:
        """Implement required compute_update method from Optimizer base class.
        
        For SMO, this is a pass-through since optimization happens in the optimize() method.
        """
        return params
        
    def optimize(self, X: Features, y: Target,
                kernel_fn: Callable, C: float) -> np.ndarray:
        X, y = check_X_y(X, y)
        n_samples = X.shape[0]
        alphas = np.zeros(n_samples)
        b = 0.0
        
        # SMO optimization loop
        for _ in range(self.max_iter):
            alpha_pairs_changed = 0
            for i in range(n_samples):
                Ei = self._decision_function(alphas, y, kernel_fn, X, X[i], b) - y[i]
                if ((y[i] * Ei < -self.tol and alphas[i] < C) or 
                    (y[i] * Ei > self.tol and alphas[i] > 0)):
                    j = self._select_second_alpha(i, n_samples)
                    alpha_pairs_changed += self._optimize_alpha_pair(
                        i, j, alphas, X, y, Ei, kernel_fn, C, b)
            
            if alpha_pairs_changed == 0:
                break
                
        return alphas

    def _select_second_alpha(self, i: int, n_samples: int) -> int:
        j = i
        while j == i:
            j = np.random.randint(0, n_samples)
        return j
        
    def _decision_function(self, alphas: np.ndarray, y: np.ndarray,
                          kernel_fn: Callable, X: np.ndarray, 
                          xi: np.ndarray, b: float) -> float:
        return np.sum(alphas * y * kernel_fn(X, xi)) + b
        
    def _optimize_alpha_pair(self, i: int, j: int, alphas: np.ndarray,
                           X: np.ndarray, y: np.ndarray, Ei: float,
                           kernel_fn: Callable, C: float, b: float) -> int:
        if i == j:
            return 0
        
        # Calculate error for second alpha
        Ej = self._decision_function(alphas, y, kernel_fn, X, X[j], b) - y[j]
        
        # Save old alphas
        old_ai, old_aj = alphas[i], alphas[j]
        
        # Compute L and H bounds
        if y[i] != y[j]:
            L = max(0, alphas[j] - alphas[i])
            H = min(C, C + alphas[j] - alphas[i])
        else:
            L = max(0, alphas[i] + alphas[j] - C)
            H = min(C, alphas[i] + alphas[j])
        
        if L == H:
            return 0
        
        # Compute kernel values
        eta = 2 * kernel_fn(X[i:i+1], X[j:j+1])[0][0]
        eta -= kernel_fn(X[i:i+1], X[i:i+1])[0][0]
        eta -= kernel_fn(X[j:j+1], X[j:j+1])[0][0]
        
        if eta >= 0:
            return 0
        
        # Update alpha j
        alphas[j] -= y[j] * (Ei - Ej) / eta
        alphas[j] = min(H, max(L, alphas[j]))
        
        if abs(alphas[j] - old_aj) < 1e-5:
            return 0
        
        # Update alpha i
        alphas[i] += y[i] * y[j] * (old_aj - alphas[j])
        
        return 1

    def _update_bias(self, alphas: np.ndarray, y: np.ndarray, 
                    kernel_fn: Callable, X: np.ndarray,
                    i: int, j: int, b: float, C: float) -> float:
        b1 = b - self._decision_function(alphas, y, kernel_fn, X, X[i], 0) + y[i]
        b2 = b - self._decision_function(alphas, y, kernel_fn, X, X[j], 0) + y[j]
        
        if 0 < alphas[i] < C:
            return b1
        elif 0 < alphas[j] < C:
            return b2
        else:
            return (b1 + b2) / 2

class StochasticGradientOptimizer(BaseOptimizer):
    """Stochastic gradient descent optimization for linear SVM."""
    
    def __init__(self, learning_rate: float = 0.01, 
                 max_iter: int = 1000,
                 batch_size: int = 32):
        super().__init__(learning_rate=learning_rate)
        self.max_iter = max_iter
        self.batch_size = batch_size
        
    def optimize(self, X: Features, y: Target,
                kernel_fn: Callable, C: float) -> np.ndarray:
        X, y = check_X_y(X, y)
        w = np.zeros(X.shape[1])
        n_samples = X.shape[0]
        
        for _ in range(self.max_iter):
            batch_idx = np.random.choice(n_samples, self.batch_size)
            X_batch = X[batch_idx]
            y_batch = y[batch_idx]
            
            # Compute gradients and update weights
            margin = y_batch * (X_batch @ w)
            mask = margin < 1
            grad = -C * (y_batch[mask].reshape(-1, 1) * X_batch[mask]).sum(axis=0)
            grad += w
            
            w -= self.learning_rate * grad
            
        return w

class OnlineSVMOptimizer(BaseOptimizer):
    """Online learning optimizer for SVM."""
    
    def __init__(self, buffer_size: int = 1000):
        super().__init__(learning_rate=0.01)
        self.buffer_size = buffer_size
        
    def optimize(self, X: Features, y: Target,
                kernel_fn: Callable, C: float) -> np.ndarray:
        # Implement online learning optimization
        pass

class SVM(Estimator):
    """Support Vector Machine classifier.
    
    Parameters
    ----------
    C : float, default=1.0
        Regularization parameter
    kernel : {'linear', 'rbf', 'poly', 'sigmoid'} or callable, default='rbf'
        Kernel function
    degree : int, default=3
        Degree for poly kernel
    gamma : float, default='scale'
        Kernel coefficient for rbf, poly and sigmoid kernels
    coef0 : float, default=0.0
        Independent term in poly/sigmoid kernels
    tol : float, default=1e-3
        Tolerance for stopping criterion
    max_iter : int, default=1000
        Maximum number of iterations
    optimizer : {'smo', 'sgd'}, default='smo'
        Optimization algorithm to use
    random_state : int, default=None
        Random number generator seed
    """
    
    def __init__(
        self,
        C: float = 1.0,
        kernel: Union[str, Callable] = 'rbf',
        degree: int = 3,
        gamma: Union[str, float] = 'scale',
        coef0: float = 0.0,
        tol: float = 1e-3,
        max_iter: int = 1000,
        optimizer: str = 'smo',
        random_state: Optional[int] = None
    ):
        super().__init__()
        self.C = C
        self.kernel = kernel
        self.degree = degree
        self.gamma = gamma
        self.coef0 = coef0
        self.tol = tol
        self.max_iter = max_iter
        self.optimizer = optimizer
        self.random_state = random_state
        
        # Initialize optimizer
        if optimizer == 'smo':
            self._optimizer = SMOOptimizer(max_iter=max_iter, tol=tol)
        elif optimizer == 'sgd':
            self._optimizer = StochasticGradientOptimizer(max_iter=max_iter)
        else:
            raise ValueError(f"Unknown optimizer: {optimizer}")
            
        # Initialize kernel function
        self._init_kernel()
        
    def _init_kernel(self):
        """Initialize the kernel function."""
        if callable(self.kernel):
            self._kernel_fn = self.kernel
        elif self.kernel == 'linear':
            self._kernel_fn = lambda X, Y: X @ Y.T
        elif self.kernel == 'rbf':
            if self.gamma == 'scale':
                self.gamma_ = 1.0  # Will be set in fit
            else:
                self.gamma_ = self.gamma
            self._kernel_fn = lambda X, Y: np.exp(-self.gamma_ * 
                np.sum((X[:, None] - Y) ** 2, axis=2))
        elif self.kernel == 'poly':
            self._kernel_fn = lambda X, Y: (self.gamma * (X @ Y.T) + 
                self.coef0) ** self.degree
        elif self.kernel == 'sigmoid':
            self._kernel_fn = lambda X, Y: np.tanh(self.gamma * 
                (X @ Y.T) + self.coef0)
        else:
            raise ValueError(f"Unknown kernel: {self.kernel}")
            
    def fit(self, X: Features, y: Target) -> 'SVM':
        """Fit the SVM model."""
        X, y = check_X_y(X, y)
        
        # Set gamma if 'scale'
        if self.kernel == 'rbf' and self.gamma == 'scale':
            self.gamma_ = 1.0 / (X.shape[1] * X.var())
            self._init_kernel()
            
        # Optimize parameters
        self.alphas_ = self._optimizer.optimize(X, y, self._kernel_fn, self.C)
        
        # Find support vectors (where alpha > EPSILON)
        support_mask = self.alphas_ > EPSILON
        if not np.any(support_mask):
            raise ValueError("No support vectors found. Try adjusting the C parameter.")
        
        # Set support vector attributes
        self.support_vectors_ = X[support_mask]
        self.support_labels_ = y[support_mask]
        self.support_alphas_ = self.alphas_[support_mask]
        
        # Compute bias term
        self.b_ = np.mean(y - self._decision_function_raw(X))
        
        # Store training data
        self.X_ = X
        self.y_ = y
        
        return self
        
    def _decision_function_raw(self, X: Features) -> np.ndarray:
        """Compute decision function without bias."""
        decisions = np.zeros(X.shape[0])
        for alpha, label, sv in zip(self.support_alphas_, 
                                   self.support_labels_,
                                   self.support_vectors_):
            decisions += alpha * label * self._kernel_fn(X, sv.reshape(1, -1))
        return decisions
        
    def decision_function(self, X: Features) -> np.ndarray:
        """Compute decision function."""
        check_is_fitted(self)
        X = check_array(X)
        
        return self._decision_function_raw(X) + self.b_
        
    def predict(self, X: Features) -> Target:
        """Predict class labels for samples in X."""
        check_is_fitted(self)
        X = check_array(X)
        
        # Compute decision function
        decision = self.decision_function(X)
        return np.sign(decision)

# Advanced kernel implementations
class AdvancedKernels:
    """Collection of advanced kernel functions."""
    
    @staticmethod
    def tanh_kernel(gamma: float, coef0: float) -> Callable:
        """Hyperbolic tangent (sigmoid) kernel."""
        return lambda X, Y: np.tanh(gamma * (X @ Y.T) + coef0)
    
    @staticmethod
    def rational_quadratic(alpha: float) -> Callable:
        """Rational quadratic kernel."""
        return lambda X, Y: 1 - np.sum((X[:, None] - Y) ** 2, axis=2) / (
            np.sum((X[:, None] - Y) ** 2, axis=2) + alpha)
            
    @staticmethod
    def wavelet_kernel(a: float, c: float) -> Callable:
        """Wavelet kernel."""
        def kernel(X: np.ndarray, Y: np.ndarray) -> np.ndarray:
            diff = X[:, None] - Y
            return np.prod(np.cos(a * diff / c) * np.exp(-np.abs(diff) / c), axis=2)
        return kernel

# Active learning strategies
class ActiveLearningStrategy(ABC):
    """Base class for active learning strategies."""
    
    @abstractmethod
    def select_samples(self, model: Estimator, X_pool: Features, 
                      n_samples: int) -> np.ndarray:
        """Select samples for labeling."""
        pass

class UncertaintySampling(ActiveLearningStrategy):
    """Uncertainty sampling strategy."""
    
    def select_samples(self, model: Estimator, X_pool: Features,
                      n_samples: int) -> np.ndarray:
        check_is_fitted(model)
        X_pool = check_array(X_pool)
        # Get decision function values
        decisions = np.abs(model.decision_function(X_pool))
        # Select samples closest to decision boundary
        return np.argsort(decisions)[:n_samples]

class DiversitySampling(ActiveLearningStrategy):
    """Diversity-based sampling strategy."""
    
    def select_samples(self, model: Estimator, X_pool: Features,
                      n_samples: int) -> np.ndarray:
        # Implement diversity-based sample selection
        pass

# Advanced SVM variants
class SparseSVM(Estimator):
    """SVM with L1 regularization for feature selection."""
    
    def __init__(self, l1_ratio: float = 0.5, **kwargs):
        super().__init__()
        self.l1_ratio = l1_ratio

class RobustSVM(Estimator):
    """Robust SVM handling outliers and noise."""
    
    def __init__(self, outlier_fraction: float = 0.1, **kwargs):
        super().__init__()
        self.outlier_fraction = outlier_fraction
        # Implement robust SVM training

class IncrementalSVM(Estimator):
    """Incremental/Online SVM learning."""
    
    def __init__(self, buffer_size: int = 1000, **kwargs):
        super().__init__()
        self.buffer_size = buffer_size
        self.optimizer = OnlineSVMOptimizer(buffer_size)
        
    def partial_fit(self, X: Features, y: Target):
        """Update model with new samples."""
        pass

# Model selection and hyperparameter optimization
class SVMModelSelector:
    """Advanced model selection and tuning."""
    
    def __init__(self, base_model: Estimator,
                 param_distributions: Dict[str, Any],
                 n_iter: int = 100,
                 cv: int = 5,
                 n_jobs: int = -1,
                 scoring: Union[str, Callable] = 'f1'):
        self.base_model = base_model
        self.param_distributions = param_distributions
        self.n_iter = n_iter
        self.cv = cv
        self.n_jobs = n_jobs
        self.scoring = scoring
        
    def select_best_model(self, X: Features, y: Target) -> Estimator:
        """Find best hyperparameters using randomized search."""
        X, y = check_X_y(X, y)
        # Implement model selection
        return self.base_model